# Activation Functions (활성화 함수)

## 1. 개요 (Overview)
Activation Function(활성화 함수)은 [신경망(Neural Networks)](./NeuralNetworks.md)의 뉴런에서 입력 신호의 총합을 출력 신호로 변환하는 함수입니다. 가장 중요한 역할은 신경망에 **비선형성(Non-linearity)**을 부여하는 것입니다.

## 2. 왜 비선형성이 필요한가?
활성화 함수가 선형(Linear)이라면, 아무리 많은 층을 쌓아도 결국 하나의 선형 함수로 표현됩니다. 즉, 복잡한 패턴이나 곡선을 학습할 수 없습니다. 비선형 함수를 사용해야 신경망이 복잡한 문제를 해결할 수 있습니다.

## 3. 주요 종류 (Types)

### 3.1. Sigmoid
- **수식**: $f(x) = \frac{1}{1 + e^{-x}}$
- **특징**: 출력을 0과 1 사이로 압축합니다. 이진 분류의 출력층에 주로 사용됩니다.
- **단점**: 입력의 절댓값이 클수록 기울기가 0에 가까워져, 학습이 멈추는 **기울기 소실(Vanishing Gradient)** 문제가 발생합니다.

### 3.2. Tanh (Hyperbolic Tangent)
- **수식**: $f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
- **특징**: 출력을 -1과 1 사이로 변환합니다. Sigmoid보다 중심이 0에 가까워 학습에 유리하지만, 여전히 기울기 소실 문제가 있습니다.

### 3.3. ReLU (Rectified Linear Unit)
- **수식**: $f(x) = \max(0, x)$
- **특징**: 입력이 양수면 그대로, 음수면 0을 출력합니다. 연산이 매우 빠르고 기울기 소실 문제가 적어, 은닉층에서 가장 많이 사용됩니다.
- **변형**: Leaky ReLU (음수일 때 약간의 기울기 허용) 등.

### 3.4. Softmax
- **특징**: 입력받은 값을 0~1 사이의 값으로 정규화하여, 출력의 총합이 1이 되도록 만듭니다. 다중 클래스 분류의 출력층에서 확률을 표현할 때 사용됩니다.
