# Linear Algebra (선형대수)

## 1. 개요 (Overview)

### 선형대수란?
Linear Algebra(선형대수)는 **벡터(Vector)**와 **행렬(Matrix)**을 다루는 수학 분야입니다. AI와 머신러닝을 이해하기 위한 가장 중요한 수학적 기초이며, **AI의 언어**라고 할 수 있습니다.

### 왜 AI에 선형대수가 필요한가?
- **데이터 표현**: 모든 데이터는 숫자 배열(벡터/행렬)로 표현됩니다
- **계산 효율성**: 행렬 연산으로 수천 개의 계산을 동시에 처리 가능
- **모델의 핵심**: 신경망은 본질적으로 행렬 곱셈의 연속입니다

**일상적 비유**:
- 스칼라 = 하나의 숫자 (예: 온도 25도)
- 벡터 = 숫자들의 리스트 (예: [키, 몸무게, 나이])
- 행렬 = 표 (예: 엑셀 스프레드시트)

## 2. 핵심 개념 (Key Concepts)

### 2.1. Scalar, Vector, Matrix, Tensor (스칼라, 벡터, 행렬, 텐서)

#### Scalar (스칼라) - 0차원
하나의 숫자입니다.
```
예: 5, 3.14, -2
```

**AI에서의 예**:
- 학습률(Learning Rate): 0.001
- 손실값(Loss): 0.52

#### Vector (벡터) - 1차원
숫자들의 배열입니다. **크기(magnitude)**와 **방향(direction)**을 가집니다.

```
예: [1, 2, 3]
    [키, 몸무게, 나이] = [175, 70, 25]
```

**AI에서의 예**:
- 한 사람의 특징(Feature Vector): [나이, 연봉, 신용점수]
- 단어 임베딩(Word Embedding): 300차원 벡터로 "사과"를 표현
- 이미지 분류 결과: [고양이 확률, 개 확률, 새 확률] = [0.7, 0.2, 0.1]

**벡터 연산**:
- **덧셈**: [1, 2] + [3, 4] = [4, 6]
- **스칼라 곱**: 2 × [1, 2] = [2, 4]
- **크기(Norm)**: ||[3, 4]|| = √(3² + 4²) = 5

#### Matrix (행렬) - 2차원
숫자들의 2차원 배열입니다. 행(row)과 열(column)로 구성됩니다.

```
예: [1 2 3]
    [4 5 6]
```

**2×3 행렬** (2개 행, 3개 열)

**AI에서의 예**:
- **데이터셋**: 각 행은 한 사람, 각 열은 특징
```
         [나이, 연봉, 신용점수]
사람1:   [25,  3000,  700]
사람2:   [30,  5000,  750]
사람3:   [35,  6000,  800]
```

- **이미지**: 흑백 이미지는 픽셀 값들의 행렬
```
28×28 흑백 이미지 = 28×28 행렬
각 원소는 0(검정)~255(흰색) 값
```

- **가중치(Weight)**: 신경망의 각 층은 가중치 행렬을 가짐
```
입력 3개 → 출력 2개 변환 = 2×3 가중치 행렬
```

#### Tensor (텐서) - 3차원 이상
벡터와 행렬을 더 높은 차원으로 확장한 개념입니다.

**예시**:
- **컬러 이미지**: 높이 × 너비 × 색상 채널(RGB)
```
224×224 컬러 이미지 = 224×224×3 텐서
```

- **동영상**: 시간 × 높이 × 너비 × 색상 = 4차원 텐서

- **배치 데이터**: 배치크기 × 높이 × 너비 × 채널 = 4차원 텐서

### 2.2. Matrix Operations (행렬 연산)

#### 행렬 덧셈
같은 크기의 행렬끼리 대응되는 원소를 더합니다.

```
[1 2] + [5 6] = [6  8]
[3 4]   [7 8]   [10 12]
```

#### Element-wise 곱셈 (Hadamard Product)
대응되는 원소끼리 곱합니다. (기호: ⊙)

```
[1 2] ⊙ [5 6] = [5  12]
[3 4]   [7 8]   [21 32]
```

#### Dot Product (내적)
두 벡터를 곱해서 **하나의 숫자**를 얻습니다.

```
[1, 2, 3] · [4, 5, 6] = 1×4 + 2×5 + 3×6 = 4 + 10 + 18 = 32
```

**의미**: 두 벡터의 **유사도** 측정
- 내적이 크면 → 벡터들이 비슷한 방향
- 내적이 0이면 → 벡터들이 직교(수직)
- 내적이 음수면 → 벡터들이 반대 방향

**AI에서의 활용**:
- 추천 시스템: 사용자 벡터와 상품 벡터의 내적으로 선호도 예측
- 코사인 유사도: 문서 간 유사도 측정

#### Matrix Multiplication (행렬 곱셈)
**가장 중요한 연산**입니다. 신경망의 핵심입니다.

**규칙**: (m×n) 행렬과 (n×p) 행렬을 곱하면 (m×p) 행렬이 나옵니다.

```
[1 2]   [5 6]   [1×5+2×7  1×6+2×8]   [19 22]
[3 4] × [7 8] = [3×5+4×7  3×6+4×8] = [43 50]
(2×2)   (2×2)         (2×2)
```

**핵심**: 왼쪽 행렬의 **행**과 오른쪽 행렬의 **열**을 내적

**AI에서의 활용** - 신경망의 층 통과:
```
입력 데이터(1×3) × 가중치(3×2) = 출력(1×2)

[x1, x2, x3] × [w11 w12]   = [y1, y2]
               [w21 w22]
               [w31 w32]

y1 = x1×w11 + x2×w21 + x3×w31
y2 = x1×w12 + x2×w22 + x3×w32
```

이것이 신경망의 **선형 변환**입니다!

#### Transpose (전치)
행과 열을 바꿉니다. (기호: A^T)

```
     [1 2 3]T   [1 4]
A =  [4 5 6]  → [2 5]
                 [3 6]
```

**AI에서의 활용**:
- 역전파(Backpropagation) 계산
- 공분산 행렬 계산: X^T × X

### 2.3. Special Matrices (특수 행렬)

#### Identity Matrix (단위 행렬)
대각선은 1, 나머지는 0인 행렬. (기호: I)

```
I = [1 0 0]
    [0 1 0]
    [0 0 1]
```

**성질**: A × I = I × A = A (숫자에서 1과 같은 역할)

#### Inverse Matrix (역행렬)
A × A^(-1) = I가 되는 행렬. (숫자에서 역수와 같은 개념)

**AI에서의 활용**:
- 최소제곱법(Least Squares) 해 구하기
- 정규 방정식(Normal Equation)

### 2.4. Eigenvalues & Eigenvectors (고유값과 고유벡터)

**정의**: 행렬 A에 대해
```
A × v = λ × v
```
를 만족하는 벡터 v(고유벡터)와 스칼라 λ(고유값)

**의미**:
- 행렬 A를 적용해도 **방향이 변하지 않는** 특별한 벡터
- 크기만 λ배로 변함

**시각적 이해**:
```
행렬은 일반적으로 벡터를 회전시키고 늘립니다.
하지만 고유벡터는 방향은 그대로, 크기만 변합니다.
```

**AI에서의 활용**:

#### 1. PCA (주성분 분석, Principal Component Analysis)
데이터의 분산이 가장 큰 방향(주성분)을 찾아 차원을 축소합니다.

**과정**:
1. 데이터의 공분산 행렬 계산
2. 공분산 행렬의 고유벡터 찾기
3. 고유값이 큰 순서대로 고유벡터 선택
4. 선택된 고유벡터 방향으로 데이터 투영

**예시**: 100차원 데이터 → 가장 중요한 10차원만 유지

#### 2. 그래프 신경망 (GNN)
그래프의 구조를 분석할 때 인접 행렬의 고유값/고유벡터 사용

## 3. AI에서의 활용 사례

### 3.1. 데이터 표현

#### 이미지
- **흑백 이미지**: 2D 행렬 (높이 × 너비)
- **컬러 이미지**: 3D 텐서 (높이 × 너비 × 3채널)
```
예: 224×224 RGB 이미지 = 224×224×3 = 150,528개 숫자
```

#### 텍스트
- **One-Hot Vector**: "사과" = [0, 0, 1, 0, ..., 0] (단어장 크기만큼)
- **Word Embedding**: "사과" = [0.2, -0.5, 0.8, ..., 0.1] (300차원 등)

#### 테이블 데이터
```
행렬의 각 행 = 한 샘플(예: 한 명의 고객)
행렬의 각 열 = 한 특징(예: 나이, 소득)
```

### 3.2. 신경망의 Forward Pass (순전파)

신경망은 본질적으로 **행렬 곱셈의 연속**입니다.

```
입력층(3) → 은닉층(4) → 출력층(2)

h = σ(X × W1 + b1)    # 입력 → 은닉층
y = σ(h × W2 + b2)    # 은닉층 → 출력층

X: 입력 (배치크기 × 3)
W1: 가중치 (3 × 4)
b1: 편향 (4)
h: 은닉층 출력 (배치크기 × 4)
W2: 가중치 (4 × 2)
b2: 편향 (2)
y: 최종 출력 (배치크기 × 2)
σ: 활성화 함수
```

### 3.3. Backpropagation (역전파)

손실 함수의 **기울기(Gradient)**를 계산할 때 행렬 미분과 Chain Rule을 사용합니다.

```
∂L/∂W2 = hT × ∂L/∂y
∂L/∂W1 = XT × (∂L/∂h)
```
모두 행렬 곱셈과 전치 연산!

### 3.4. 차원 축소

#### PCA (주성분 분석)
```
1. 데이터 중심화: X_centered = X - mean(X)
2. 공분산 행렬: Cov = (1/n) × X_centeredT × X_centered
3. 고유값 분해: Cov × v = λ × v
4. 상위 k개 고유벡터 선택하여 투영
```

#### t-SNE, UMAP
고차원 데이터를 2D/3D로 시각화할 때도 선형대수 사용

### 3.5. Batch Processing (배치 처리)

행렬 연산으로 여러 데이터를 **동시에** 처리합니다.

```
한 번에 하나씩:
for each sample:
    output = sample × weight  # 느림!

배치로 한 번에:
outputs = samples × weight  # 빠름! (GPU 활용)

samples: (배치크기 × 입력차원)
weight: (입력차원 × 출력차원)
outputs: (배치크기 × 출력차원)
```

GPU는 행렬 연산에 최적화되어 있어 수백 배 빠릅니다!

## 4. 실전 팁

### 차원 맞추기
행렬 곱셈에서 가장 흔한 오류는 **차원 불일치**입니다.

```
(m × n) × (n × p) = (m × p)  ✓ 가능
(m × n) × (k × p) = 오류!     ✗ n ≠ k
```

**디버깅 팁**:
```python
print(f"X shape: {X.shape}")        # (100, 3)
print(f"W shape: {W.shape}")        # (3, 4)
print(f"output: {X @ W .shape}")    # (100, 4)
```

### Broadcasting (브로드캐스팅)
NumPy/PyTorch는 자동으로 차원을 확장해줍니다.

```python
X = [[1, 2], [3, 4]]  # (2, 2)
b = [10, 20]          # (2,)

X + b = [[11, 22], [13, 24]]  # b가 자동으로 [[10, 20], [10, 20]]으로 확장
```

### 효율적인 계산
```python
# 느린 방법: for loop
result = 0
for i in range(len(a)):
    result += a[i] * b[i]

# 빠른 방법: 벡터 내적
result = np.dot(a, b)  # 수백 배 빠름!
```

## 5. 더 공부하면 좋은 주제

### 초급
- ✓ 벡터/행렬 덧셈, 곱셈
- ✓ 전치 행렬
- ✓ 내적의 의미

### 중급
- SVD (Singular Value Decomposition, 특이값 분해)
- QR 분해
- 놈(Norm)의 종류: L1, L2, Frobenius

### 고급
- 텐서 분해 (Tensor Decomposition)
- 행렬 미분 (Matrix Calculus)
- 리만 기하학과 다양체 (Manifold)

## 6. 관련 문서
- [Calculus (미적분)](./Calculus.md) - 기울기 계산에 필요
- [Probability and Statistics (확률과 통계)](./ProbabilityStatistics.md) - 공분산 행렬 등
- [Machine Learning Basics (머신러닝 기초)](./MLBasic.md) - 선형대수 활용
- [Deep Learning Basics (딥러닝 기초)](./DLBasic.md) - 행렬 연산의 실전
- [Neural Networks (신경망)](./NeuralNetworks.md) - 선형대수의 연속
- [Dimensionality Reduction (차원 축소)](./DimensionalityReduction.md) - PCA, SVD 활용
